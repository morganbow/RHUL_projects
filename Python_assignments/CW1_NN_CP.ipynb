{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Nearest Neighbour and Conformal Predicition applied to the Iris dataset from sklearn and the Ionoshphere dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "# ~~~~~~\n",
    "### 1. Functions for the assignment\n",
    "### 2.a Iris data 1NN\n",
    "### 2.b Ionosphere data 1NN\n",
    "### 3.a Iris data conformal prediction\n",
    "### 3.b Ionosphere data conformal prediciton\n",
    "### 4 Results\n",
    "# ~~~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are two functions that I will use in both the algorithms. min_finder will find the minimum value in a list or an array and argmin_finder will find the index of the minimum value in a list or array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def min_finder(x):\n",
    "    \"\"\"Finds the minimunm value of the list or array that is inputed\"\"\"\n",
    "    min = math.inf\n",
    "    for i in range (len(x)):\n",
    "        if x[i] < min:\n",
    "            min = x[i]\n",
    "    return min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_finder(x):\n",
    "    \"\"\"Finds the minimunm value of the list or array that is inputed\"\"\"\n",
    "    min = math.inf\n",
    "    argmin = 0\n",
    "    for i in range (len(x)):\n",
    "        if x[i] < min:\n",
    "            min = x[i]\n",
    "            argmin = i\n",
    "    return argmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### euc_dis will be used in both algorithms to find the Euclidean distances between two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def euc_dis(x,y):\n",
    "    \"\"\"Return the Euclidean distance of the inputted vectors\"\"\"\n",
    "    euc_calc = np.empty(len(x))\n",
    "    for i in range(len(x)):\n",
    "        euc_calc[i] =(x[i] - y[i]) ** 2\n",
    "    ans = np.sum(euc_calc) ** 0.5\n",
    "    return(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris data 1NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I imported the iris dataest and split it using sklearns train_test_split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=1504)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I then calculated the distances from every test point to every training point and put them all in the 2D array called distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.empty(len(X_test)*len(X_train)).reshape(len(X_test),len(X_train))\n",
    "for i in range(len(X_test)):\n",
    "    for j in range(len(X_train)):\n",
    "        distances[i,j] = euc_dis(X_test[i],X_train[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally I've predicted the label for each test data point based on the label given to the training point that is closest to it according to Euclidean distance. Then I've calculated how many of these predictions differ from the test data's actual labels to get the error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.empty(len(X_test))\n",
    "for i in range (len(X_test)):\n",
    "    y_pred[i] = y_train[argmin_finder(distances[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05263157894736842"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_count = sum(y_pred!=y_test)\n",
    "error_rate = (error_count/len(X_test))\n",
    "error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1NN iris dataset test eror rate = 0.0526 (3.s.f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ionoshpere data 1NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I loaded in the Ionosphere dataset using the np.genfromtxt and split the loaded data into its feature data and target data. The proccess after this is much the same as for the Iris dataest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ionosphere_data = np.genfromtxt('ionosphere.txt', delimiter=',', usecols=np.arange(34))\n",
    "ionosphere_target = np.genfromtxt('ionosphere.txt', delimiter=',', usecols=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xio_train, Xio_test, yio_train, yio_test = train_test_split(ionosphere_data, ionosphere_target, random_state=1504)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_io = np.empty(len(Xio_test)*len(Xio_train)).reshape(len(Xio_test),len(Xio_train))\n",
    "for i in range(len(Xio_test)):\n",
    "    for j in range(len(Xio_train)):\n",
    "        distances_io[i,j] = euc_dis(Xio_test[i],Xio_train[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "yio_pred = np.empty(len(Xio_test))\n",
    "for i in range (len(Xio_test)):\n",
    "    yio_pred[i] = yio_train[argmin_finder(distances_io[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11363636363636363"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_count_io = sum(yio_pred!=yio_test)\n",
    "error_rate_io = (error_count_io/len(Xio_test))\n",
    "error_rate_io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1NN ionosphere dataset test error rate = 0.114 (3.s.f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris data Conformal Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firstly I created a 3D matrix that contains every possible extended training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_train = np.empty((len(X_train)+1)*4*len(X_test)).reshape(len(X_test),(len(X_train)+1),4)\n",
    "for i in range (len(X_test)):\n",
    "    big_train[i] = np.r_[X_train,X_test[i].reshape(1,4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the code below I have looped through every extended training dataset and for each version I have calculated the distance from every point to every other point, making sure to not include the distance to itself as this would be 0 and cause problems further down the line. All of this is saved in the 3D matrix called big_distances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_distances = np.empty(len(big_train)*len(big_train[0])*len(big_train[0])).reshape(len(big_train),len(big_train[0]),(len(big_train[0])))\n",
    "for i in range (len(big_train)):\n",
    "    for j in range (len(big_train[0])):\n",
    "        for k in range (len(big_train[0])):\n",
    "            if j != k:\n",
    "                big_distances[i,j,k] = euc_dis(big_train[i,j],big_train[i,k])\n",
    "            else:\n",
    "                big_distances[i,j,k] = math.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next I turned this 3D matrix into a 4D matrix where each individual scalar represents either the minimum distance to the same class or the minimum distance to a different class of one of the data points on the extended training datasets where each test point on every extended training dataset either has the label  Y = {0,1,2}.\n",
    "\n",
    "### For example min_dist_same_dif_class[0,0,0,0] represents the closest point of the same class to the first point in the extended training dataset where the test point that has been added to it is given the label 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train0 = np.append(y_train,0)\n",
    "y_train1 = np.append(y_train,1)\n",
    "y_train2 = np.append(y_train,2)\n",
    "min_dist_same_dif_class = np.empty(2*3*len(big_train)*(len(big_train[0]))).reshape(2,3,len(big_train),(len(big_train[0])))\n",
    "\n",
    "for ds in range (2):\n",
    "    for a in range (3):\n",
    "        for b in range (len(big_train)):\n",
    "            for c in range (len(big_train[0])):\n",
    "                if ds == 0:\n",
    "                    if a == 0:\n",
    "                        min_dist_same_dif_class[ds,a,b,c] = min_finder(big_distances[b,c][y_train0 == y_train0[c]])\n",
    "                    elif a == 1:\n",
    "                        min_dist_same_dif_class[ds,a,b,c] = min_finder(big_distances[b,c][y_train1 == y_train1[c]])\n",
    "                    else:\n",
    "                        min_dist_same_dif_class[ds,a,b,c] = min_finder(big_distances[b,c][y_train2 == y_train2[c]])\n",
    "                else:\n",
    "                    if a == 0:\n",
    "                        min_dist_same_dif_class[ds,a,b,c] = min_finder(big_distances[b,c][y_train0 != y_train0[c]])\n",
    "                    elif a == 1:\n",
    "                        min_dist_same_dif_class[ds,a,b,c] = min_finder(big_distances[b,c][y_train1 != y_train1[c]])\n",
    "                    else:\n",
    "                        min_dist_same_dif_class[ds,a,b,c] = min_finder(big_distances[b,c][y_train2 != y_train2[c]])            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have looped through the min_dist_same_dif_class matrix to calculate the alpha for every single point of all the different extended training sets. \n",
    "### In this loop I have accounted for the standard definitions of a/0 = $inf$ and 0/0 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.empty(3*len(big_train)*(len(big_train[0]))).reshape(3,len(big_train),(len(big_train[0])))\n",
    "for i in range (3):\n",
    "    for j in range (len(big_train)):\n",
    "        for k in range (len(big_train[0])):\n",
    "            if min_dist_same_dif_class[0,i,j,k] == 0:\n",
    "                if min_dist_same_dif_class[1,i,j,k] == 0:\n",
    "                    alpha[i,j,k] = 0\n",
    "                else:\n",
    "                    alpha[i,j,k] = np.inf\n",
    "            else:\n",
    "                alpha[i,j,k] = min_dist_same_dif_class[1,i,j,k]/min_dist_same_dif_class[0,i,j,k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To find the rank I have looped through every extended training data sets alpha's comparing them to all other alpha's of the same extended training dataset and I have increased the rank if the alpha is greater than or equal to another alpha. \n",
    "### This works because when deciding upon the rank in conformal prediciton if the two points are equal then the rank is considered to be the higher of the two. i.e if all alpha's were equal the rank would be the highest possible rank, n+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = np.ones(3*len(big_train)*(len(big_train[0]))).reshape(3,len(big_train),(len(big_train[0]))) \n",
    "\n",
    "for i in range (3):\n",
    "    for j in range (len(big_train)):\n",
    "        for k in range (len(alpha[0,0])):\n",
    "            for l in range (len(alpha[0,0])):\n",
    "                if k != l:\n",
    "                    if alpha[i,j,k] >= alpha[i,j,l]:\n",
    "                        rank[i,j,k] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally I have calculated the p-values for each different labelled test data point in every extended data set. \n",
    "### I've then used these p-values to calculate the average false p-value making sure to discount the true labels p-value instead of just the highest scoring p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = np.empty(len(big_train)*3).reshape(len(big_train),3)\n",
    "\n",
    "for i in range (len(big_train)):\n",
    "    for j in range (3):\n",
    "        p_val[i,j] = rank[j,i,-1]/len(big_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_p_val = 0\n",
    "for i in range (len(big_train)):\n",
    "    false_p_val += np.sum(p_val[i]) - p_val[i,y_test[i]]\n",
    "average_false_p_val = false_p_val/(len(big_train)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011411271541686084"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_false_p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris dataset average false p-value for the Nearest Neighbour conformal predictor = 0.0114 (3.s.f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ionosphere data conformal prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I carried out conformal prediction for the Ionosphere dataset in a very similar manner, the main alternative is that I used a second conformity measure 1/(distance to nearest sample of same class) as well as the one used previously to compare results. I'll state any other differences in the algorithm as they appear.\n",
    "### The first slight difference being the size of the dimensions of the dataset will clearly be slighly different as seen below when creating the array of extended datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_train_io = np.empty((len(Xio_train)+1)*34*len(Xio_test)).reshape(len(Xio_test),(len(Xio_train)+1),34)\n",
    "for i in range (len(Xio_test)):\n",
    "    big_train_io[i] = np.r_[Xio_train,Xio_test[i].reshape(1,34)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_distances_io = np.empty(len(big_train_io)*len(big_train_io[0])*len(big_train_io[0])).reshape(len(big_train_io),len(big_train_io[0]),(len(big_train_io[0])))\n",
    "for i in range (len(big_train_io)):\n",
    "    for j in range (len(big_train_io[0])):\n",
    "        for k in range (len(big_train_io[0])):\n",
    "            if j != k:\n",
    "                big_distances_io[i,j,k] = euc_dis(big_train_io[i,j],big_train_io[i,k])\n",
    "            else:\n",
    "                big_distances_io[i,j,k] = math.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due to there only being two labels in the Ionospere target dataset I only need to loop over two different potential labels for the test data points in each extended training dataset, instead of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "yio_train_neg = np.append(yio_train,-1)\n",
    "yio_train_pos = np.append(yio_train,1)\n",
    "min_dist_same_dif_class_io = np.empty(2*2*len(big_train_io)*(len(big_train_io[0]))).reshape(2,2,len(big_train_io),(len(big_train_io[0])))\n",
    "\n",
    "for ds in range (2):\n",
    "    for a in range (2):\n",
    "        for b in range (len(big_train_io)):\n",
    "            for c in range (len(big_train_io[0])):\n",
    "                if ds == 0:\n",
    "                    if a == 0:\n",
    "                        min_dist_same_dif_class_io[ds,a,b,c] = min_finder(big_distances_io[b,c][yio_train_neg == yio_train_neg[c]])\n",
    "                    else:\n",
    "                        min_dist_same_dif_class_io[ds,a,b,c] = min_finder(big_distances_io[b,c][yio_train_pos == yio_train_pos[c]])\n",
    "                else:\n",
    "                    if a == 0:\n",
    "                        min_dist_same_dif_class_io[ds,a,b,c] = min_finder(big_distances_io[b,c][yio_train_neg != yio_train_neg[c]])\n",
    "                    else:\n",
    "                        min_dist_same_dif_class_io[ds,a,b,c] = min_finder(big_distances_io[b,c][yio_train_pos != yio_train_pos[c]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next few blocks of code show the two diffferent calculations of alpha. The first block uses the measure used previously which is:\n",
    "(distance to the nearest sample of different class)/(distance to the nearest sample of same class)\n",
    "### The second block of code uses the alternative conformity measure of:\n",
    "1/(distance to the nearest value of same class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_io = np.empty(2*len(big_train_io)*(len(big_train_io[0]))).reshape(2,len(big_train_io),(len(big_train_io[0])))\n",
    "for i in range (2):\n",
    "    for j in range (len(big_train_io)):\n",
    "        for k in range (len(big_train_io[0])):\n",
    "            if min_dist_same_dif_class_io[0,i,j,k] == 0:\n",
    "                if min_dist_same_dif_class_io[1,i,j,k] == 0:\n",
    "                    alpha_io[i,j,k] = 0\n",
    "                else:\n",
    "                    alpha_io[i,j,k] = np.inf\n",
    "            else:\n",
    "                alpha_io[i,j,k] = min_dist_same_dif_class_io[1,i,j,k]/min_dist_same_dif_class_io[0,i,j,k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_alpha_io = np.empty(2*len(big_train_io)*(len(big_train_io[0]))).reshape(2,len(big_train_io),(len(big_train_io[0])))\n",
    "for i in range (2):\n",
    "    for j in range (len(big_train_io)):\n",
    "        for k in range (len(big_train_io[0])):\n",
    "            if min_dist_same_dif_class_io[0,i,j,k] == 0:\n",
    "                alt_alpha_io[i,j,k] = np.inf\n",
    "            else:\n",
    "                alt_alpha_io[i,j,k] = 1/min_dist_same_dif_class_io[0,i,j,k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ranks of the alpha's of the original conformity measure and the alternative are both computed. These are used to calculate the different p-value's and subsequently the two average false p-value's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_io = np.ones(2*len(big_train_io)*(len(big_train_io[0]))).reshape(2,len(big_train_io),(len(big_train_io[0]))) \n",
    "\n",
    "for i in range (2):\n",
    "    for j in range (len(big_train_io)):\n",
    "        for k in range (len(alpha_io[0,0])):\n",
    "            for l in range (len(alpha_io[0,0])):\n",
    "                if k != l:\n",
    "                    if alpha_io[i,j,k] >= alpha_io[i,j,l]:\n",
    "                        rank_io[i,j,k] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_rank_io = np.ones(2*len(big_train_io)*(len(big_train_io[0]))).reshape(2,len(big_train_io),(len(big_train_io[0]))) \n",
    "\n",
    "for i in range (2):\n",
    "    for j in range (len(big_train_io)):\n",
    "        for k in range (len(alt_alpha_io[0,0])):\n",
    "            for l in range (len(alt_alpha_io[0,0])):\n",
    "                if k != l:\n",
    "                    if alt_alpha_io[i,j,k] >= alt_alpha_io[i,j,l]:\n",
    "                        alt_rank_io[i,j,k] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val_io = np.empty(len(big_train_io)*2).reshape(len(big_train_io),2)\n",
    "\n",
    "for i in range (len(big_train_io)):\n",
    "    for j in range (2):\n",
    "        p_val_io[i,j] = rank_io[j,i,-1]/len(big_train_io[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_p_val_io = np.empty(len(big_train_io)*2).reshape(len(big_train_io),2)\n",
    "\n",
    "for i in range (len(big_train_io)):\n",
    "    for j in range (2):\n",
    "        alt_p_val_io[i,j] = alt_rank_io[j,i,-1]/len(big_train_io[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To be able to find the correct p-values index I had to change all the -1 labels to 0 as these 0 labels can be used as an index to subtract the correct p-value's of each test data point from the sum of the p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(yio_test)):\n",
    "    if yio_test[i] == -1:\n",
    "        yio_test[i] = 0\n",
    "false_p_val_io = 0\n",
    "for i in range (len(big_train_io)):\n",
    "    false_p_val_io += np.sum(p_val_io[i]) - p_val_io[i,int(yio_test[i])]\n",
    "average_false_p_val_io = false_p_val_io/(len(big_train_io))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05720557851239672"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_false_p_val_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_false_p_val_io = 0\n",
    "for i in range (len(big_train_io)):\n",
    "    alt_false_p_val_io += np.sum(alt_p_val_io[i]) - alt_p_val_io[i,int(yio_test[i])]\n",
    "alt_average_false_p_val_io = alt_false_p_val_io/(len(big_train_io))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2870179063360882"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_average_false_p_val_io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ionosphere dataset average false p-value for the Nearest Neighbour conformal predictor = 0.0572 (3.s.f)\n",
    "\n",
    "Using the alternate conformity measure the average false p-value = 0.287 (3.s.f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "(all to 3 significanct figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the test error rate for Nearest Neighbour applied to iris.txt = 0.0526\n",
    "* the test error rate for Nearest Neighbour applied to ionosphere.txt = 0.114\n",
    "* the average false p-value for the Nearest Neighbour conformal predictor applied to iris.txt = 0.0114\n",
    "* the average false p-value for the Nearest Neighbour conformal predictor applied to ionosphere.txt = 0.0572\n",
    "* the average false p-value for the Nearest Neighbour alternative conformal predictor applied to ionosphere.txt = 0.287"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
